{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# OpenAI API 基本使用\n",
    "\n",
    "关键参数：\n",
    "\n",
    "- model，使用哪个模型，这些就填入哪个模型的名称\n",
    "- messages，发给模型的消息，这里的消息是一个消息列表，可以简单地把它理解成一个历史的消息列表，这样模型就知道了上下文信息。\n",
    "    - 模型记忆的本质，就是这里的历史消息列表，让模型有记忆，就是按下面对 message 格式的介绍，把用户（user）和 AI(assistant) 的对话信息放到历史消息列表总\n",
    "    - 列表中的每一项，都是一个 message，每条消息通常包括两个部分：角色（role）和内容（content）\n",
    "    - role 常用的有三个：`system`, `user`和`assistant`\n",
    "        - system，给 AI 设定的身份、背景信息等，通常放到 system 信息中\n",
    "        - user，我们发给 AI 的信息，放到 user 信息中\n",
    "        - assistant，AI 给我们的回复信息，放到 assistant 信息中\n",
    "- n，控制 AI 给我们生成回复的个数\n",
    "- temperature，用来设定大模型回复的确定性，值越小，表示确定性越强，值越大，表示随机性越强，取值范围[0, 2], 通常取值 0.8\n",
    "- max_tokens，控制返回内容的最大 token 数量\n",
    "\n",
    "另外，还有一个与 temperature 作用类似的参数：`top_p`，也是用来设定大模型回复的确定性，值越小，表示确定性越强，值越大，表示随机性越强，取值范围[0, 2]，它与`temperature` 只有填入一个即可\n",
    "\n"
   ],
   "id": "d10380e50429b204"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T13:57:18.768677Z",
     "start_time": "2024-11-30T13:57:13.675501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are an IT expert. You can help me by answering my questions. You can also ask me questions.\"},\n",
    "        {\"role\": \"user\", \"content\": \"用一句话介绍微服务架构。\"},\n",
    "    ],\n",
    "    n=1,\n",
    "    temperature=0.8,\n",
    "    max_tokens=256\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "2bcb37a962239f1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微服务架构是一种将应用程序拆分为一组小型、独立的服务，每个服务负责特定功能，通过轻量级的通信协议进行交互，从而提高系统的灵活性和可维护性。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# OpenAI API 返回数据的结构化输出\n",
    "\n",
    "OpenAI 的 completion 接口，可以通过`response_format` 参数，指定返回数据的格式。\n",
    "\n",
    "以往该参数的输入值为: `{ \"type\": \"json_object\" }`，即 “JSON mode”，该方式需要配合提示词工程，在提示词中约定好输出 json 数据的格式，但不能严格保证返回数据一定按提示词的 json 格式生成。\n",
    "\n",
    "现在，OpenAI 较新的模型开始支持 `Structured Outputs` 形式的输出，即不需借助提示词，直接保证大模型严格按照约定的 Json 格式进行输出，详见官方文档：[Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction)\n",
    "\n",
    "\n",
    "支持 Structured Outputs 模型, 从 GPT-4o 开始，具体为:\n",
    "\n",
    "- gpt-4o-mini-2024-07-18 and later\n",
    "- gpt-4o-2024-08-06 and later\n",
    "\n",
    "\n",
    "注意， `Structured Outputs` 对 python 和 openai 版本均有要求，使用的方法也不同：\n",
    "\n",
    "- python >= 3.12.4\n",
    "- openai >=1.40.0\n",
    "- 使用 `client.beta.chat.completions.parse()`方法\n",
    "\n"
   ],
   "id": "9ae3f3a1419cdbe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T12:51:11.990743Z",
     "start_time": "2024-12-01T12:51:09.756108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Structured Outputs Example\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "]\n",
    "\n",
    "# 原来的写法：\n",
    "# response = client.chat.completions.create(\n",
    "#     model=model_name,\n",
    "#     messages=msgs,\n",
    "#     temperature=0.8,\n",
    "#     n=1,\n",
    "# )\n",
    "# msg = response.choices[0].message\n",
    "# print(\"result: \", msg.content)\n",
    "\n",
    "# Structured Outputs\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=model_name,\n",
    "    messages=msgs,\n",
    "    temperature=0.8,\n",
    "    n=1,\n",
    "    response_format=CalendarEvent\n",
    ")\n",
    "\n",
    "msg = response.choices[0].message\n",
    "print(\"content:\", msg.content, \"\\ntype: \", type(msg.content))\n",
    "\n",
    "parsed = msg.parsed\n",
    "print(\"parsed object: \", parsed, \"\\ntype: \", type(parsed))"
   ],
   "id": "c26dcffaa498324f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: {\"name\":\"Science Fair\",\"date\":\"Friday\",\"participants\":[\"Alice\",\"Bob\"]} \n",
      "type:  <class 'str'>\n",
      "parsed object:  name='Science Fair' date='Friday' participants=['Alice', 'Bob'] \n",
      "type:  <class '__main__.CalendarEvent'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:37:21.954225Z",
     "start_time": "2024-12-01T15:37:18.979712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON mode Example\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You extract email addresses into JSON data.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Feeling stuck? Send a message to help@mycompany.com.\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"email_schema\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"email\": {\n",
    "                        \"description\": \"The email address that appears in the input\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content);"
   ],
   "id": "29d05d2dcb10fed3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"email\":\"help@mycompany.com\"}\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:39:39.822579Z",
     "start_time": "2024-12-01T15:39:36.247952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON mode Example 2\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You extract email addresses into JSON data, output should be a json string.\n",
    "            For example, as for abc@email.com, your output should be: '{\"email\":\"help@mycompany.com\"}'.\n",
    "            Constraints: you should output json string only, do not use markdown format\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Feeling stuck? Send a message to help@mycompany.com.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content);"
   ],
   "id": "41b49b3543e87df4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"email\":\"help@mycompany.com\"}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:26:23.962086Z",
     "start_time": "2024-12-01T13:26:14.102769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Structured Outputs -- Chain of Thought Example\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_output: str\n",
    "\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a helpful math tutor. Guide the user through the solution step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"how can I solve 8x + 7 = -25\"}\n",
    "    ],\n",
    "    response_format=MathReasoning\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.parsed)\n"
   ],
   "id": "1a242dc5e892d85c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"steps\":[{\"explanation\":\"To isolate the term with x, start by subtracting 7 from both sides of the equation.\",\"output\":\"8x + 7 - 7 = -25 - 7\"},{\"explanation\":\"Simplifying both sides gives you: 8x = -32.\",\"output\":\"8x = -32\"},{\"explanation\":\"Next, to solve for x, divide both sides of the equation by 8.\",\"output\":\"x = -32 / 8\"},{\"explanation\":\"Calculating the division gives you: x = -4.\",\"output\":\"x = -4\"}],\"final_output\":\"The solution to the equation 8x + 7 = -25 is x = -4.\"}\n",
      "steps=[Step(explanation='To isolate the term with x, start by subtracting 7 from both sides of the equation.', output='8x + 7 - 7 = -25 - 7'), Step(explanation='Simplifying both sides gives you: 8x = -32.', output='8x = -32'), Step(explanation='Next, to solve for x, divide both sides of the equation by 8.', output='x = -32 / 8'), Step(explanation='Calculating the division gives you: x = -4.', output='x = -4')] final_output='The solution to the equation 8x + 7 = -25 is x = -4.'\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:43:06.521148Z",
     "start_time": "2024-12-01T13:42:55.362193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 内容审查\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class Category(str, Enum):\n",
    "    violence = \"violence\"\n",
    "    sexual = \"sexual\"\n",
    "    self_harm = \"self_harm\"\n",
    "\n",
    "\n",
    "class ContentCompliance(BaseModel):\n",
    "    is_violating: bool\n",
    "    category: Optional[Category]\n",
    "    explanation_if_violence: Optional[str]\n",
    "\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"Determine if the user input violates specific guidelines and explain if they do.\"},\n",
    "        {\"role\": \"user\", \"content\": \"How do I prepare for a job interview?\"}\n",
    "    ],\n",
    "    response_format=ContentCompliance,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "2cfcc4a74d00bb42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"is_violating\":false,\"category\":null,\"explanation_if_violence\":null}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 大模型的记忆\n",
    "\n",
    "大模型记忆的本质，就是把与大模型的对话历史，也放到 messages 里去，一并发给大模型，这样在本次的对话中，大模型就知道之前都说了什么。\n"
   ],
   "id": "9cfe9a1d77ab1dad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:12:56.313624Z",
     "start_time": "2024-12-03T13:12:25.921602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a helpful assistant\"}\n",
    "]\n",
    "\n",
    "def make_message(role, content) -> dict:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You:> \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    print(\"You:> \", user_input)\n",
    "    msg = make_message(\"user\", user_input)\n",
    "    msgs.append(msg)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"you are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    assistant_msg = resp.choices[0].message.content\n",
    "    print(\"AI: \", assistant_msg)\n",
    "\n"
   ],
   "id": "4493df2e43e8e0d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:>  我周五去看电影\n",
      "AI:  那听起来很不错！你打算看什么电影呢？或者你已经选好影院了吗？\n",
      "You:>  我刚才说周五去干什么？\n",
      "AI:  抱歉，我无法查看之前的对话内容。请告诉我你提到的周五要做什么，我会尽力帮助你！\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:10:22.445750Z",
     "start_time": "2024-12-03T13:09:53.055604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a helpful assistant\"}\n",
    "]\n",
    "\n",
    "def make_message(role, content) -> dict:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You:> \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    print(\"You:> \", user_input)\n",
    "    msg = make_message(\"user\", user_input)\n",
    "    msgs.append(msg)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=msgs,\n",
    "        temperature=0.8,\n",
    "        max_tokens=256\n",
    "    )\n",
    "    assistant_msg = resp.choices[0].message.content\n",
    "    print(\"AI: \", assistant_msg)\n",
    "    msg = make_message(\"assistant\", assistant_msg)\n",
    "    msgs.append(msg)\n",
    "\n"
   ],
   "id": "86ba267279801f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:>  我周五晚上去看电影\n",
      "AI:  听起来不错！你要看什么电影呢？有没有特别期待的影片或者类型？\n",
      "You:>  我周五晚上去干什么？\n",
      "AI:  你提到周五晚上去看电影，所以可以说你计划去看电影。如果你有其他安排或者想做的事情，也可以分享出来！\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 流式输出",
   "id": "90b0f913df67b2a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:05:44.297637Z",
     "start_time": "2024-12-01T15:05:36.485289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "msgs = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"我希望你充当 IT 专家。我会向您提供有关我的技术问题所需的所有信息，而您的职责是解决我的问题。你应该使用你的项目管理知识，敏捷开发知识来解决我的问题。在您的回答中使用适合所有级别的人的智能、简单和易于理解的语言将很有帮助。用要点逐步解释您的解决方案很有帮助。我希望您回复解决方案，而不是写任何解释。\"},\n",
    "    {\"role\": \"user\", \"content\": \"解释微服务架构\"}\n",
    "]\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=msgs,\n",
    "    n=1,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is None:\n",
    "        break\n",
    "    # print(chunk.choices[0].delta.content, end=\"\")\n",
    "    print(chunk.choices[0].delta.content, end=\"/\")"
   ],
   "id": "d7497474d1443d4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/微/服务/架/构/是一/种/软件/架/构/风/格/，/旨/在/将/单/一/应用/程序/划/分/为/一/组/小/的/、/独/立/的/服务/。/以下/是/微/服务/架/构/的/主要/特点/和/优势/：\n",
      "\n",
      "/1/./ **/服务/独/立/性/**/：\n",
      "/  / -/ 每/个/微/服务/都是/一个/独/立/的/功能/模块/，可以/独/立/开发/、/部署/和/扩/展/。\n",
      "/  \n",
      "/2/./ **/技术/多/样/性/**/：\n",
      "/  / -/ 不/同/的/服务/可以/使用/不同/的/编/程/语言/、/框/架/和/数据库/。\n",
      "/  \n",
      "/3/./ **/容/错/性/**/：\n",
      "/  / -/ 单/个/服务/的/故/障/不会/影响/整/个平台/，其/它/服务/仍/可以/继续/运行/。\n",
      "/  \n",
      "/4/./ **/持续/交/付/与/部署/**/：\n",
      "/  / -/ 支/持/快速/迭/代/，/能够/实现/频/繁/的/更新/和/发布/。\n",
      "\n",
      "/5/./ **/团队/自治/**/：\n",
      "/  / -/ 每/个/团队/可以/对/其/服务/负责/，从/而/提升/开发/效率/和/推动/更/快/的/决/策/。\n",
      "\n",
      "/6/./ **/API/ /交/互/**/：\n",
      "/  / -/ 服务/之间/通过/轻/量/级/的/通信/机制/（/通常/是/ HTTP///REST/ 或/消息/队/列/）/进行/交/互/。\n",
      "\n",
      "/7/./ **/易/于/扩/展/**/：\n",
      "/  / -/ 可以/根据/服务/的/需求/独/立/扩/展/某/些/服务/，而/不是/整体/扩/展/整个/应用/。\n",
      "\n",
      "/8/./ **/监/控/与/管理/**/：\n",
      "/  / -/ /需要/有效/的/监/控/和/管理/工具/，以/处理/服务/间/的/复杂/性/和/可/追/踪/性/。\n",
      "\n",
      "/**/总结/**/：/微/服务/架/构/通过/将/应用/程序/拆/分/为/小/的/独/立/服务/，/提供/了/灵/活/性/、/可/扩/展/性/和/恢复/力/，/适/合/现代/云/原/生/开发/。///"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Function calling\n",
    "\n",
    "\n"
   ],
   "id": "c535c2d46d861cee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:30:18.890224Z",
     "start_time": "2024-12-03T03:30:16.679003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "# define a tool\n",
    "def get_current_cluster_state(cluster_name):\n",
    "    print(f\"cluster:{cluster_name}\")\n",
    "    return \"\"\"ERROR: Failed to pull image \"chaocai/docker/dsp:latest\"\"\"\n",
    "\n",
    "\n",
    "# tool description for LLM\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_cluster_state\",\n",
    "            \"description\": \"Get the current state in a given cluster\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"cluster_name\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"the name of the cluster\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"cluster_name\"],  # 配置必填参数\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's wrong with the cluster 'DSP'?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # `auto` is the default if tools are present\n",
    ")\n",
    "\n",
    "# 执行 function call 时，tool_calls 有值，且 content 中没有值\n",
    "assert response.choices[0].message.tool_calls is not None\n",
    "assert response.choices[0].message.content is None\n",
    "assert len(response.choices[0].message.tool_calls) > 0\n"
   ],
   "id": "51ee7a01be21ac25",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:30:28.694499Z",
     "start_time": "2024-12-03T03:30:28.691852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tool_calls = response.choices[0].message.tool_calls\n",
    "print(len(tool_calls))\n",
    "print(tool_calls[0].function.name)  # 函数名\n",
    "print(tool_calls[0].function.arguments)  # 函数的参数和值，json 字符串格式"
   ],
   "id": "cf9cfd3a99f7f5b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "get_current_cluster_state\n",
      "{\"cluster_name\":\"DSP\"}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:30:35.328983Z",
     "start_time": "2024-12-03T03:30:35.326405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(response.choices[0].message.role)\n",
    "print(response.choices[0].message)"
   ],
   "id": "6b1eb24f82ec3505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Ps8ZNpWtV47pUXkNLobU0prs', function=Function(arguments='{\"cluster_name\":\"DSP\"}', name='get_current_cluster_state'), type='function', index=0)])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:18:39.120255Z",
     "start_time": "2024-12-03T03:18:39.117139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 根据 function 结果进行函数调用\n",
    "\n",
    "import json\n",
    "\n",
    "tools_map = {\n",
    "    \"get_current_cluster_state\": get_current_cluster_state\n",
    "}\n",
    "\n",
    "tool_name = tool_calls[0].function.name\n",
    "args = json.loads(tool_calls[0].function.arguments)\n",
    "\n",
    "tool = tools_map[tool_name]\n",
    "\n",
    "res = tool(**args)\n",
    "print(res)"
   ],
   "id": "cf506b51a8c718fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:DSP\n",
      "ERROR: Failed to pull image \"chaocai/docker/dsp:latest\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T03:21:36.261262Z",
     "start_time": "2024-12-03T03:21:33.544307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 当大模型判断不需要进行 function call 时，会自己回答问题\n",
    "\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"你是谁?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # `auto` is the default if tools are present\n",
    ")\n",
    "\n",
    "# 不执行 function call 时，tool_calls 没有值，且 content 中有值\n",
    "assert response.choices[0].message.tool_calls is None\n",
    "assert response.choices[0].message.content is not None\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "17ec03d3f9f12c5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个人工智能助手，旨在回答你的问题、提供信息和帮助你解决各种问题。有什么我可以为你做的吗？\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Function call 进阶用法\n",
    "\n",
    "当我们让 LLM 执行完 Function call 后，除了调用工具执行结果外，有时我们还希望再把工具调用完的结果反馈给 LLM，让大模型做进一步判断后续的执行。\n",
    "\n",
    "实现这个目标，本质是把 function call 的结果加到 LLM 的记忆中去，这里的记忆和文本内容的记忆有些许差别，详情如下："
   ],
   "id": "c0a642aab731c41d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T06:27:28.244002Z",
     "start_time": "2024-12-03T06:27:28.231333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function call + 记忆\n",
    "\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletionMessage\n",
    "\n",
    "client = OpenAI()\n",
    "model_name = \"openai/gpt-4o-mini\"\n",
    "\n",
    "\n",
    "def chat(messages: list) -> ChatCompletionMessage:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tool_choice=\"auto\",\n",
    "        tools=tools,\n",
    "    )\n",
    "    msg = response.choices[0].message\n",
    "\n",
    "    if not msg.tool_calls:\n",
    "        return msg\n",
    "\n",
    "    tool_call = msg.tool_calls[0]\n",
    "    func_name = tool_call.function.name\n",
    "    func_args = tool_call.function.arguments\n",
    "\n",
    "    res = call_tool(func_name, func_args)\n",
    "    messages.append({\n",
    "        \"role\": msg.role,\n",
    "        # deprecated\n",
    "        \"function_call\": {\n",
    "            \"name\": func_name,\n",
    "            \"arguments\": func_args,\n",
    "        },\n",
    "        \"content\": None\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"function\",\n",
    "        \"name\": func_name,\n",
    "        \"content\": res,\n",
    "    })\n",
    "\n",
    "    return chat(messages)\n",
    "\n",
    "def chat2(messages: list) -> ChatCompletionMessage:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tool_choice=\"auto\",\n",
    "        tools=tools,\n",
    "    )\n",
    "    msg = response.choices[0].message\n",
    "\n",
    "    if not msg.tool_calls:\n",
    "        return msg\n",
    "\n",
    "    tool_call = msg.tool_calls[0]\n",
    "    func_name = tool_call.function.name\n",
    "    func_args = tool_call.function.arguments\n",
    "\n",
    "    res = call_tool(func_name, func_args)\n",
    "    messages.append({\n",
    "        \"role\": msg.role,\n",
    "        # deprecated\n",
    "        \"function_call\": {\n",
    "            \"name\": func_name,\n",
    "            \"arguments\": func_args,\n",
    "        },\n",
    "        \"content\": None\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_call.id,\n",
    "        \"content\": res,\n",
    "    })\n",
    "\n",
    "    return chat(messages)\n",
    "\n",
    "def chat3(messages: list) -> ChatCompletionMessage:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        tool_choice=\"auto\",\n",
    "        tools=tools,\n",
    "    )\n",
    "    msg = response.choices[0].message\n",
    "\n",
    "    if not msg.tool_calls:\n",
    "        return msg\n",
    "\n",
    "    tool_call = msg.tool_calls[0]\n",
    "    func_name = tool_call.function.name\n",
    "    func_args = tool_call.function.arguments\n",
    "\n",
    "    res = call_tool(func_name, func_args)\n",
    "    messages.append({\n",
    "        \"role\": msg.role,\n",
    "        \"tool_calls\": [{\n",
    "            \"id\": tool_call.id,\n",
    "            \"function\": tool_call.function,\n",
    "            \"type\": \"function\"\n",
    "        }],\n",
    "        \"content\": None\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_call.id,\n",
    "        \"content\": res,\n",
    "    })\n",
    "\n",
    "    return chat(messages)\n",
    "\n",
    "\n",
    "def call_tool(tool_name: str, tool_args: str):\n",
    "    tool = tools_map[tool_name]\n",
    "    args = json.loads(tool_args)\n",
    "\n",
    "    return tool(**args)\n",
    "\n",
    "def run(inputs):\n",
    "    msg=[{\"role\":\"user\",\"content\":inputs}]\n",
    "    res = chat3(msg)\n",
    "    return res.content"
   ],
   "id": "d852bfec5cd107f4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T06:24:10.396336Z",
     "start_time": "2024-12-03T06:24:08.530651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = run(\"hello\")\n",
    "print(res)"
   ],
   "id": "abd41bf3beae2447",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T06:37:49.659876Z",
     "start_time": "2024-12-03T06:37:42.215571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = run(\"What's wrong with the cluster 'DSP'? \")\n",
    "print(result)"
   ],
   "id": "f69dfc982c44d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:DSP\n",
      "The cluster 'DSP' is encountering an error because it failed to pull the image \"chaocai/docker/dsp:latest\". This could be due to various reasons such as the image not being available, network issues, or permission problems.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T06:27:37.844409Z",
     "start_time": "2024-12-03T06:27:31.416075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = run(\"What's wrong with the cluster 'DSP'? And if there's an error, give me some suggestion. \")\n",
    "print(result)"
   ],
   "id": "b720b79f88a883da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster:DSP\n",
      "The current issue with the cluster 'DSP' is that it failed to pull the Docker image \"chaocai/docker/dsp:latest.\" This type of error can happen for several reasons. Here are some suggestions to resolve the issue:\n",
      "\n",
      "1. **Check Internet Connectivity**: Ensure that the cluster has access to the internet, as it needs to connect to the Docker registry to pull the image.\n",
      "\n",
      "2. **Image Availability**: Verify if the image \"chaocai/docker/dsp:latest\" exists in the Docker registry. You can do this by visiting Docker Hub or the relevant registry where the image is supposed to be hosted.\n",
      "\n",
      "3. **Authentication Issues**: If the image is in a private repository, ensure that the correct authentication credentials are provided.\n",
      "\n",
      "4. **Check for Typos**: Double-check the image name and tag for any typos.\n",
      "\n",
      "5. **Resource Limits**: Ensure that the cluster has enough resources (CPU, memory, etc.) to pull and run the image.\n",
      "\n",
      "6. **Docker Daemon Status**: Ensure that the Docker daemon is running properly on the nodes of the cluster.\n",
      "\n",
      "7. **Pull Manually**: Try to pull the image manually using Docker CLI on one of the nodes in the cluster to see if you can get more detailed error messages.\n",
      "\n",
      "Attempt these actions to diagnose and resolve the issue with the cluster.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "890f764a5d1cc6a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
